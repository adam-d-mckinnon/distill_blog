[
  {
    "path": "posts/2020-12-30-causalimpactanalysis/",
    "title": "Show Me The money! A People Analytics Guide to Measuring Impact Over Time",
    "description": "A \"How To\" article on the use of Causal Impact Analysis for People Analytics Practitioners.",
    "author": [
      {
        "name": "Adam D McKinnon",
        "url": "https://adam-d-mckinnon.com/"
      }
    ],
    "date": "2020-12-30",
    "categories": [
      "Causal Impact Analysis",
      "Impact Measurement",
      "Plotly",
      "R"
    ],
    "contents": "\n\nContents\nShow Me The Money!\nCausal Impact Analysis\nHow Does It Work?\n\nEnough Theory More Practice!\n1. Create Time Series Data\n2. Perform Causal Impact Analysis Using UK Data & Review Results\n3. Perform Causal Impact Analysis Using UK And Control Data & Review Results\n4. Quantifying The Financial Impact Of The Campaign\n\nHmmm Interesting But… Now What?\n\n\n\n\n(#fig:image_header)Image source: Cubankite, Africa Studio, Dean Drobot - Shutterstock.\n\n\n\nShow Me The Money!\nWords immortalised by Tom Cruise in the 1996 move Jerry Maguire, yet at the heart of requests directed toward HR to quantify impact. However, quantifying impact is not always easy in HR—a field characterised by a sparsity of measurable KPIs such as product sales, conversions, defect rates, etc. While HR may not have the same measures, it can nonetheless borrow methods from neighbouring fields that deal in behavioural change and possess greater analytical maturity. To that end, this article is intended to introduce Causal Impact Analysis—a method initially developed in product marketing—to demonstrate a way to quantify impact in HR.\nCausal Impact Analysis\nCausal Impact Analysis is an approach to estimate “the causal effect of a designed intervention on a time series” (https://google.github.io/CausalImpact/CausalImpact.html). A HR related example would be the following—does a marketing campaign intended to get employees in the UK to reduce leave balances have an impact? Addressing questions like this can be challenging when controlled experimental conditions are not available. Kay Brodersen and team at Google built this algorithm to address this very challenge, and kindly open-sourced it for the rest of us to benefit from.\nHow Does It Work?\nLet’s use the HR example above–-does a marketing campaign launched in the UK that is intended to get employees to reduce leave balances have an impact? Causal Impact Analysis works by:\nexamining UK employee absences over time (i.e., a time series);\nexamining the absences in other, ideally comparable countries (i.e., a control time series); and\nbuilds a time-series model to try and predict how employee absences in the UK should have evolved without the campaign. The time series model can be built using either: a) only UK absence data, or b), using both UK data and the control time series (i.e., comparable countries).\nEnough Theory More Practice!\nThe following code provides a walkthrough of Causal Impact Analysis using R. The example context described above (i.e., quantifying the impact of a marketing campaign launched in the UK intended to get employees to reduce leave balances), will be played out below using fictitious data. The code plays out in the following way:\ncreate time series data\nperform causal impact analysis using UK data & review results\nperform causal impact analysis using UK and control data (i.e., employee absence in Italy, Sweden, Spain and Germany) & review results\nquantifying the financial impact of the campaign\n1. Create Time Series Data\nThe below code stub generates fictitious data for our example. Please note that you can replicate this example, however, the data generated may be different, which in turn may lead to different outcomes (i.e., both visualisations, summary statistics and benefit quantified).\n\n\n# load libraries\n\nlibrary(tidyverse) # workhorse\nlibrary(janitor) # better naming conventions\nlibrary(lubridate) # working with dates\nlibrary(zoo) # time series data format for causal impact\nlibrary(CausalImpact) # our method of interest\nlibrary(plotly) # making graphs attractive and interactive\n\n\nset.seed(1)\n\n# create a date sequence\n# takes default timezone setting from machine\nmydatetime <- as.POSIXct(\"2018-01-01\") \ndate <- base::seq.POSIXt(from = mydatetime, length.out = 365, by = \"day\")\n\n\n# control data - 4 countries (Italy, Sweden, Spain, Germany)\n# numbers reflect number of employees on leave each day\nit_numbers <- 100 + stats::arima.sim(model = list(ar = 0.999), n = 365) \nse_numbers <- 1.5 * it_numbers + rnorm(85)\nes_numbers <- 0.8 * it_numbers + rnorm (25)\nde_numbers <- 2.5 * it_numbers + rnorm (125)\n\n\n# case country data - UK\n# implementing a gradual increase to reflect realistic behaviour \n# (i.e., incremental absence/leave taking increase)\nuk_numbers          <- 1.2 * it_numbers + rnorm(100)\nuk_numbers[275:295] <- uk_numbers[275:365] + 2\nuk_numbers[296:316] <- uk_numbers[275:365] + 5\nuk_numbers[317:347] <- uk_numbers[275:365] + 12\nuk_numbers[348:365] <- uk_numbers[275:365] + 10\n\n\n# combine the data into one tibble\ndata_tbl <- bind_cols(date, uk_numbers, it_numbers, se_numbers, es_numbers, de_numbers) %>% \n            janitor::clean_names() %>% \n            dplyr::rename(date = x1,\n                   uk_numbers = x2,\n                   it_numbers = x3,\n                   se_numbers = x4,\n                   es_numbers = x5,\n                   de_numbers = x6) %>% \n            \n            # ONLY INCLUDE WEEKDAY DATA - no absences on weekend!\n            dplyr::mutate(date2 = date %>% lubridate::as_date() %>% lubridate::wday()) %>% \n\n            # 1 = sunday and 7 = saturday, 2 - 6 = weekdays\n            dplyr::filter(date2 >1 & date2 < 7) %>% \n\n            #  round numbers to whole numbers (no employee parts!)\n            dplyr::mutate_at(vars(dplyr::ends_with(\"_numbers\")), ~ base::round(., 0)) %>% \n            dplyr::select(-date2) \n\n\n# create UK only data\nuk_data <- data_tbl %>% \n            dplyr::select(1:2) %>% \n            base::as.data.frame() %>% \n            zoo::read.zoo()\n\n                              \n# create all countries data\nall_country_data <- data_tbl %>% \n                    base::as.data.frame() %>% \n                    zoo::read.zoo() \n\n\n\nWe now have our employee absence data for both the UK and our control countries of Italy, Sweden, Spain and Germany. Let’s begin by assessing the impact of the campaign when looking at the UK data in isolation.\n2. Perform Causal Impact Analysis Using UK Data & Review Results\nWe begin by specifying the pre and post campaign periods. With this established we are able to perform the causal impact analysis and then both visualise results and provide a statistical summary of the model.\n\n\n# establish when the intervention occurred\npre.period <- as.POSIXct(c(\"2018-01-01\", \"2018-09-30\"))\npost.period <- as.POSIXct(c(\"2018-10-01\", \"2018-12-31\"))\n\n\n# perform the causal impact analysis with UK data only\nimpact_uk_only <- CausalImpact::CausalImpact(data         = uk_data, \n                                              pre.period  = pre.period, \n                                              post.period = post.period)\n\n\n# get a graphical summary of the UK only-causal impact model\ngraphics::plot(impact_uk_only) %>% \n    plotly::plotly_build()\n\n\npreserve52dafc329dd16a19\n\nThe graphical visualisation of the model provides three key perspectives of the model. The top panel of the visualisation shows a blue confidence interval, in which we would expect our time series data to remain in the absence of the campaign. The second panel displays the difference between situation normal (i.e., no campaign), which is depicted by the the zero value line in the graph, and what happened in reality. In our example we see the pointwise values occur both above and below the zero line. The third panel provides a perspective on the cumulative benefit of the campaign. The visualisation adds the pointwise contributions from the second panel, showing the benefit was minimal and only realised at the end of the post intervention period. None of these visualisation panels inspire confidence in our campaign!\n\n\n# get a summary &/or report of the model\nbase::summary(impact_uk_only) # summary results\n\n\nPosterior inference {CausalImpact}\n\n                         Average          Cumulative     \nActual                   107              7039           \nPrediction (s.d.)        106 (0.66)       6988 (43.74)   \n95% CI                   [104, 107]       [6895, 7070]   \n                                                         \nAbsolute effect (s.d.)   0.77 (0.66)      51.07 (43.74)  \n95% CI                   [-0.47, 2.2]     [-31.26, 144.0]\n                                                         \nRelative effect (s.d.)   0.73% (0.63%)    0.73% (0.63%)  \n95% CI                   [-0.45%, 2.1%]   [-0.45%, 2.1%] \n\nPosterior tail-area probability p:   0.11503\nPosterior prob. of a causal effect:  88%\n\nFor more details, type: summary(impact, \"report\")\n\n# plain text walk through of result - not run\n# base::summary(impact_uk_only, \"report\") # summary report\n\n\n\nThe summary results do not paint a compelling picture for our campaign either. I will call out a few key results and couple this with text lifted directly from the Summary Report (not included in this article due to its length, but a very useful explanation for Causal Impact Analysis users). The summary indicates that during the post-intervention period we had an actual average of 107 employees on leave per day. In the absence of the campaign we would have expected to have an average 106 employees on leave per day. This expectation is based on a time series model generated using pre-campaign UK data.\nOur 95% confidence interval tells us we would need the number to fall above 107 (or below 105 in case the campaign had an adverse impact) to be considered to have had an impact. The summary report goes into considerably more detail, and includes the following “although the intervention appears to have caused a positive effect, this effect is not statistically significant when considering the entire post-intervention period as a whole.” This interpretation is confirmed by our Posterior tail-area probability of p = 0.146. This means that the positive effect noted in the results may be by chance and is not considered statistically significant.\n3. Perform Causal Impact Analysis Using UK And Control Data & Review Results\nLet’s repeat the process, though this time including the data from the control countries (i.e., Italy, Sweden, Spain, and Germany. With our data, and pre and post campaign periods already established we can proceed directly to modelling and examining the output of the model.\n\n\n# perform the causal impact analysis with all data\nimpact_all_data <- CausalImpact::CausalImpact(data = all_country_data, \n                                              pre.period  = pre.period, \n                                              post.period = post.period)\n\n\n# get a graphical summary of the campaign impact\ngraphics::plot(impact_all_data) %>% \n    plotly::plotly_build()\n\n\npreserve2b7b0febf1aa813c\n\nThis time the visualisation of the model results are more compelling! The top panel shows a blue line which is our baseline. The baseline indicates where we would expect UK data to be in the absence of the campaign (based on both the control countries and UK data). The actual data appears to exceed the baseline. The second panel, depicting the difference between no campaign (i.e., the zero line) and the actual data indicated the campaign had a positive impact. The third panel suggests that the cumulative benefit of the campaign, based on the pointwise contributions from the second panel, is considerable. The campaign appears to be working, but let’s get a statistical perspective.\n\n\n# get a summary &/or report of the model\nbase::summary(impact_all_data) # summary results\n\n\nPosterior inference {CausalImpact}\n\n                         Average        Cumulative   \nActual                   107            7039         \nPrediction (s.d.)        100 (0.36)     6600 (23.90) \n95% CI                   [99, 101]      [6555, 6646] \n                                                     \nAbsolute effect (s.d.)   6.7 (0.36)     439.1 (23.90)\n95% CI                   [6, 7.3]       [393, 484.3] \n                                                     \nRelative effect (s.d.)   6.7% (0.36%)   6.7% (0.36%) \n95% CI                   [6%, 7.3%]     [6%, 7.3%]   \n\nPosterior tail-area probability p:   0.001\nPosterior prob. of a causal effect:  99.8997%\n\nFor more details, type: summary(impact, \"report\")\n\n# plain text walkthrough of result - not run\n# base::summary(impact_all_data, \"report\") # summary report\n\n\n\nThe results of the model suggest that the campaign had a positive impact that is statistically significant! Our actual average of 107 employees being on leave each day is above that of the model prediction, which was 100. In addition, our actual average is well above our 95% Confidence Interval of 101. Finally the probability that this result was due to chance alone is 0.001, and we have achieved a 99.8997% probability that the campaign had a causal effect (i.e., worth betting on!).\nComparing the two models (i.e., UK data only vs. UK data and control countries) clearly illustrates the value of including control data in this instance.\n4. Quantifying The Financial Impact Of The Campaign\nBased on the period we analysed (i.e., 1/10/2018 - 31/12/2018) our second model indicates that under normal circumstances we would have expected 100 employees on average to take leave each day, while the campaign seems to have catalysed a daily average of 107 employees. The cumulative benefit of this campaign is an additional 439 UK employees taking leave during the post campaign period (i.e., subtracting our cumulative prediction (6599) from our cumulative actual (7036)).\nIf the average daily cost of an UK employee is $300 GBP we could simply determine the financial benefit of the campaign by multiplying 439 by our average daily cost of an employee (i.e., $300 GBP). The estimated gross financial benefit of the campaign would be $131,700 GBP, which can be narrowed to a net financial benefit of $101,700 GBP, after subtracting our campaign costs of $30,000 GBP. Not bad HR!\nHmmm Interesting But… Now What?\nIn using Causal Impact Analysis, we can both:\nmeasure the ROI (Return-on-Investment) of behavioural campaigns (e.g. marketing campaign), especially when we are not sure if the campaign was the only single source of the impact; and\npotentially use Causal Impact Analysis to get feedback on impact quickly so that adjustments can be made in the moment, as opposed to when campaign momentum has been lost.\nIn addition, when tied with outcome metrics (e.g., average cost of unused leave) we can quantify the impact of interventions to demonstrate ROI to internal stakeholders. Causal Impact Analysis can be employed in a variety of settings—HR, Marketing, Communications, Finance, etc. The current article is intended to both provide a simple overview of the process of applying Causal Impact Analysis, and act as catalyst to future innovative applications in the People Analytics domain. Happy measuring!\n\n\n\n",
    "preview": "posts/2020-12-30-causalimpactanalysis/causalimpactanalysis_files/makeitrain.jpg",
    "last_modified": "2021-01-03T09:09:18+11:00",
    "input_file": "causalimpactanalysis.utf8.md"
  },
  {
    "path": "posts/2020-10-10-forecastpeopleanalytics/",
    "title": "What is the Forecast for People Analytics?",
    "description": "An article on the use of Google Trends and the new TimeTK and ModelTime Libraries in R.",
    "author": [
      {
        "name": "Adam D McKinnon",
        "url": "https://adam-d-mckinnon.com/"
      },
      {
        "name": "Monica Ashton",
        "url": {}
      }
    ],
    "date": "2020-10-10",
    "categories": [
      "Forecasting",
      "TimeTK",
      "ModelTime",
      "Plotly",
      "R"
    ],
    "contents": "\n\nContents\nThe goal is to know what’s coming…\nData Collection\nModeling\nStep 1.\nStep 2\nStep 3.\nStep 4.\n\nImplications of Forecasting in HR\nWorkforce Planning\nTalent Acquisition\nOutsourcing\nFinancial budgeting\nAcknowledgments\n\n\n\n\n\nFigure 1: Photo by Chris Liverani on Unsplash.\n\n\n\nThe last 9 months have, more than ever, emphasized the importance of knowing what is coming. In this article, we take a closer look at forecasting. Forecasting can be applied to a range of HR-related topics. We will specifically examine how forecasting models can be deployed in R, using an example analysis on the rise in popularity of “people analytics”.\nThe goal is to know what’s coming…\nPredictions come in different shapes and sizes. There are many Supervised Machine Learning algorithms that can generate predictions of outcomes, such as flight risk, safety incidents, performance and engagement outcomes, and personnel selection. These examples represent the highly popular realm of “Predictive Analytics”.\nHowever, a less mainstream topic in the realm of prediction is that of “Forecasting” – often referred to as Time Series Analysis. In a nutshell, forecasting takes values over time (e.g., closing price of a stock over 120 days) to forecast the likely value in the future.\nThe main difference between supervised machine learning and forecasting is best characterized by the data used. Generally, forecasting relies upon historical data, and the patterns identified therein, to predict future values.\nAn HR-related example would be using historical rates of attrition in a business or geography to forecast future rates of attrition. In contrast, predictive analytics uses a variety of additional variables, such as company performance metrics, economic indicators, employment data, and so on, to predict future rates of turnover. Depending upon the use case, there is a time and a place for both approaches.\nIn the current article, we focus on forecasting and highlight a new library in the R ecosystem called ModelTime. ModelTime enables the application of multiple forecasting models quickly and easily while employing a tidy framework (for those not familiar with R don’t worry about this).\nTo illustrate the ease of using ModelTime we forecast the future level of interest in the domain of People Analytics using Google Trends data. From there we will discuss potential applications of forecasting supply and demand in the context of HR.\nData Collection\nThe time-series data we will use for our example comes directly from Google Trends. Google Trends is an online tool that enables users to discover trends in search behavior within Google Search, Google News, Google Images, Google Shopping, and YouTube.\nTo do so, users are required to specify the following:\nA search term (up to four additional comparison search terms are optional),\nA geography (i.e., where the Google Searches were performed),\nA time period, and\nGoogle source for searches (e.g., Web Search, Image Search, News Search, Google Shopping, or YouTube Search).\nIt is important to note that the search data returned does NOT represent the actual search volume in numbers, but rather a normalized index ranging from 0-100. The values returned represent the search interest relative to the highest search interest during the time period selected. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular at that point in time. A score of 0 means there was not enough data for this term.\n\n\n# Libraries\nlibrary(gtrendsR)\nlibrary(tidymodels)\nlibrary(modeltime)\nlibrary(tidyverse)\nlibrary(timetk)\nlibrary(lubridate)\nlibrary(flextable)\nlibrary(prophet)\n\n\n# Data - Google Trends Setup\nsearch_term   <- \"people analytics\"\nlocation      <- \"\" # global\ntime          <- \"2010-01-01 2020-08-01\" # format \"Y-m-d Y-m-d\"\ngprop         <- \"web\"\n\n\n# Get Google Trends Data\ngtrends_result_list <- gtrendsR::gtrends(\n    keyword = search_term, \n    geo     = location, \n    time    = time,\n    gprop   = gprop\n    )\n\n\n# Data Cleaning\ngtrends_search_tbl <- gtrends_result_list %>%\n    purrr::pluck(\"interest_over_time\") %>%\n    tibble::as_tibble() %>%\n    dplyr::select(date, hits) %>%\n    dplyr::mutate(date = ymd(date)) %>%\n    dplyr::rename(value = hits)\n\n\n# Visualise the Google Trends Data\ngtrends_search_tbl %>%\n  timetk::plot_time_series(date, value)\n\n\npreservebfb1301d30e3bc1e\n\nWe can see from the visualisation that the term “people analytics” has trended upwards in Google web searches from January 2010 through to August 2020. The blue trend line, established using a LOESS smoother (i.e., a non-parametric technique that tries to find a curve of best fit without assuming the data adheres to a specific distribution) illustrates a continual rise in interest. The raw data also indicates that the Google search term of “people analytics”, perhaps unsurprisingly, peaked in June of 2020.\nThis peak may relate to the impact of COVID-19, specifically the requirement for organisations to deliver targeted ad-hoc reporting on personnel topics during this time. Irrespective, the future for People Analytics seems to be of increasing importance.\nModeling\nLet’s move into some Forecasting! The process employed using ModelTime is as follows:\nWe separate our dataset into “Training” and “Test” datasets. The Training data represents that data from January 2010 to July 2019, while the Test data represents the last 12 months of data (i.e., August 2019 – August 2020). A visual representation of this split is presented in the image you see below.\nThe Training data is used to generate a 12-month forecast using several different models. In this article, we have chosen the following models: Exponential Smoothing, ARIMA, ARIMA Boost, Prophet, and Prophet Boost.\nThe forecasts generated are then compared to the Test data (i.e., actual data) to determine the accuracy of the different models.\nBased on the accuracy of the different models, one or more models are then applied to the entire dataset (i.e., Jan 2010 – August 2020) to provide a forecast into 2021.\n\n\n\nWe have presented the R code below, with supporting outputs, for steps 1 through to 4.\nStep 1.\n\n\n# Train/Test \n# hold out 12 months for test split\nmonths <- 12\n\n\n# calculate the total numnber of months in data\ntotal_months <- lubridate::interval(base::min(gtrends_search_tbl$date),\n                                    base::max(gtrends_search_tbl$date)) %/%  \n                                    base::months(1)\n\n\n# determine the proportion for the test split\nprop <- (total_months - months) / total_months\n\n\n# Train/Test Split\nsplits <- rsample::initial_time_split(gtrends_search_tbl, prop = prop)\n\n\n# Plot splits as sanity check\nsplits %>%\n  timetk::tk_time_series_cv_plan() %>%  \n  timetk::plot_time_series_cv_plan(date, value) \n\n\npreserve4779d5934102266c\n\nThe plot above visually depicts our Training and Testing splits in the data, specifically the time period represented by both.\nStep 2\nWe first generate a 12-month forecast using five models (Exponential Smoothing, ARIMA, ARIMA Boost, Prophet, and Prophet Boost) on the training data.\n\n\n# Modeling\n# Exponential Smoothing\nmodel_fit_ets <- modeltime::exp_smoothing() %>%\n    parsnip::set_engine(engine = \"ets\") %>%\n    parsnip::fit(value ~ date, data = training(splits))\n\n\n# ARIMA \nmodel_fit_arima <- modeltime::arima_reg() %>%\n    parsnip::set_engine(\"auto_arima\") %>%\n    parsnip::fit(\n        value ~ date, \n        data = training(splits))\n\n\n# ARIMA Boost\nmodel_fit_arima_boost <- modeltime::arima_boost() %>%\n    parsnip::set_engine(\"auto_arima_xgboost\") %>%\n    parsnip::fit(\n        value ~ date + as.numeric(date) + month(date, label = TRUE), \n        data = training(splits))\n\n\n# Prophet\nmodel_fit_prophet <- modeltime::prophet_reg() %>%\n    parsnip::set_engine(\"prophet\") %>%\n    parsnip::fit(\n        value ~ date, \n        data = training(splits))\n\n\n# Prophet Boost\nmodel_fit_prophet_boost <- modeltime::prophet_boost() %>%\n    parsnip::set_engine(\"prophet_xgboost\") %>%\n    parsnip::fit(\n        value ~ date + as.numeric(date) + month(date, label = TRUE), \n        data = training(splits))\n\n\n# Modeltime Table\nmodel_tbl <- modeltime::modeltime_table(\n    model_fit_ets,\n    model_fit_arima,\n    model_fit_arima_boost,\n    model_fit_prophet,\n    model_fit_prophet_boost)\n\n\n\nStep 3.\nThe forecasts generated above are now compared to the Test data (i.e., actual data) to determine the accuracy of the different models.\n\n\n# Calibrate the model accuracy using the test data\ncalibration_tbl <- model_tbl %>%\n    modeltime::modeltime_calibrate(testing(splits))  \n\n\n# create a table of the calibration results\ncalibration_tbl %>%\n    modeltime::modeltime_accuracy() %>%   \n    flextable::flextable() %>% \n    flextable::bold(part = \"header\") %>% \n    flextable::bg(bg = \"#D3D3D3\", part = \"header\") %>% \n    flextable::autofit()\n\n\npreservea3b9c639cd6dea68\n\nThe table above illustrates the metrics derived when evaluating the accuracy of the respective models using the Test set. While it is beyond the scope of this article to explain the models and their metrics, a simple rule of thumb when looking at the below table is that smaller error numbers generally indicate a better model!\nThe graph below illustrates how the models performed relative to the actual data (i.e., our Test set).\n\n\n# plot the forecast\ncalibration_tbl %>%\n    modeltime::modeltime_forecast(new_data = testing(splits), \n                                  actual_data = gtrends_search_tbl,\n                                  conf_interval = 0.90) %>%\n    modeltime::plot_modeltime_forecast(.legend_show = TRUE, \n                                       .legend_max_width = 20)\n\n\npreserve6225d2723d79bb0d\n\nStep 4.\nBased on these metrics we decided to use all five models to forecast into 2021 (i.e., our final Step 4).\n\n\n# Refit the five models with the last 12 months of data (i.e., Test data)\nrefit_tbl <- calibration_tbl %>%\n    modeltime::modeltime_refit(data = gtrends_search_tbl) \n\n\n# forecast the next 12 months into 2021\nforecast_tbl <- refit_tbl %>%\n    modeltime::modeltime_forecast(\n        h = \"1 year\",\n        actual_data = gtrends_search_tbl,\n        conf_interval = 0.90\n    ) \n\n\n# 3 create an interactive visualization of the forecast\nforecast_tbl %>%\n    modeltime::plot_modeltime_forecast(.interactive = TRUE,\n                                       .legend_max_width = 20)\n\n\npreserve70c72fd669adaab0\n\nTo enhance the accuracy and interpretability (particularly among stakeholders) of the forecast we will take an average of the five models to create an aggregate model. We can see below in both the code and the interactive visualization, that the ongoing trend for people analytics is one of increasing popularity over time!\n\n\n# Create an aggregated model based on our 5 models\nmean_forecast_tbl <- forecast_tbl %>%\n    dplyr::filter(.key != \"actual\") %>%\n    dplyr::group_by(.key, .index) %>%\n    dplyr::summarise(across(.value:.conf_hi, mean)) %>%\n    dplyr::mutate(\n        .model_id   = 6,\n        .model_desc = \"AVERAGE OF MODELS\"\n    )\n\n\n# Visualize aggregate model \nforecast_tbl %>%\n    dplyr::filter(.key == \"actual\") %>%\n    dplyr::bind_rows(mean_forecast_tbl) %>%\n    modeltime::plot_modeltime_forecast()  \n\n\npreserve92dd27e12f9d2062\n\nThe forecast of Google Search interest for the next 12 months appears to continue its trend of upward growth – the forecast for People Analytics seems bright!\nImplications of Forecasting in HR\nThe above example illustrates the ease with which analysts can perform forecasting in R with time-series data to be better prepared for the future. In addition, the use of automated models (i.e., those that self-optimize) can be an excellent entry point for forecasting. Technologies such as ModelTime in R enable users to rapidly and easily apply numerous sophisticated forecasting models to perform scenario planning within organisations.\nScenario planning need not be something that is performed once and later shelved to collect dust, despite varying environmental conditions. In the realm of HR, forecasting can and should be readily repeated to play a crucial, and yet often-underutilized part of strategic activities such as the following:\nWorkforce Planning\nWhat proportion of the employee population is likely to retire over the coming 2 – 5 years? How many employees will the organization need to replace in the future?\nWill the local job market or universities “produce” sufficient candidates to cover an organisations forecasted graduate/ professional employee recruitment needs?\nWhen opening new facilities in new markets, is the local population sufficient to support our employee requirements?\nAre there job profile areas where we are likely to experience talent shortfalls in the near to medium-term future?\nWhat is the trend in specific skills as captured by online job boards? Of the skills that your organisation values / requires, what do you have, what do you forecast needing in light of varying environmental conditions?\nTalent Acquisition\nHow many employees are we likely to recruit in the next 2 – 4 quarters to meet business goals?\nHow many talent acquisition staff will be required in specific geographies to meet seasonal recruiting requirements (which do vary by geography!)?\nOutsourcing\nBased on the historical outsourcing activities, what is the current trend and what are the financial implications associated with that requirement?\nWill the outsourcing provider be able to cater to future demand requirements? Request forecasts from vendors to substantiate their proposals.\nBased on turnover among outsourced roles, what are the future implications for onboarding and training needs in specific businesses/geographies? How many L&D staff will be required to support those demands?\nFinancial budgeting\nWhat are the future budget requirements for HR activities?\nWhat is the future financial requirement associated with establishing a people analytics team? ;-)\nThe above list, while far from comprehensive, provides a sense of the multitude of ways in which forecasting can be applied in HR to make data driven decisions.\nHappy Forecasting!\nAcknowledgments\nThe authors would like to acknowledge the work of Matt Dancho, both in the development and maintenance of TimeTK and ModelTime, and the Learning Labs Pro Series ran by Matt, upon which this article is directly based.\nThis article was first published on the Analytics In HR (AIHR) website under the title “Forecasting in R: a People Analytics Tool” on August 31st, 2020.\n\n\n\n",
    "preview": "posts/2020-10-10-forecastpeopleanalytics/chris-liverani-dBI_My696Rk-unsplash.jpg",
    "last_modified": "2021-01-03T09:04:31+11:00",
    "input_file": "forecastpeopleanalytics.utf8.md"
  },
  {
    "path": "posts/2020-08-04-clusteranalysis/",
    "title": "A People Analytics Tutorial on Unsupervised Machine Learning - Cluster Analysis in R",
    "description": "A \"How To\" article on the use of Cluster Analysis to understand phenomenon within Organisations.",
    "author": [
      {
        "name": "Adam D McKinnon",
        "url": "https://adam-d-mckinnon.com/"
      },
      {
        "name": "Monica Ashton",
        "url": {}
      }
    ],
    "date": "2020-08-04",
    "categories": [
      "Cluster Analysis",
      "Gowers Distance",
      "TSNE",
      "Plotly",
      "R"
    ],
    "contents": "\n\nContents\nCluster Analysis in HR\n1. Importing the Data\n2. Selecting Variables for Clustering\n3. Analysis: Gower Distance\n4. Analysis: How many Clusters?\n5. Analysis: Interpreting the Clusters\nPersona’s\n\nA final note\nAcknowledgments\n\n\n\n\n\nFigure 1: Image source: Clustering: scikit-learn.org\n\n\n\nWe recently published an article titled “A Beginner’s Guide to Machine Learning for HR Practitioners” where we touched on the three broad types of Machine Learning (ML); reinforcement, supervised, and unsupervised learning. In this follow-up article we will explore unsupervised ML in more depth. We will demonstrate how we use cluster analysis, a form of unsupervised ML, to identify similarities, patterns, and relationships in datasets intelligently (like humans – but faster or more accurately). We have included some practical code written in R. Let’s get started!\nCluster Analysis in HR\nThe objective we aim to achieve is an understanding of factors associated with employee turnover within our data. To do this, we form clusters based on a set of employee variables (i.e., Features) such as age, marital status, role level, and so on. The clusters help us better understand the many attributes that may be associated with turnover, and whether there are distinct clusters of employees that are more susceptible to turnover. This last insight can facilitate the personalising of the employee experience at scale by determining whether current HR policies are serving the employee clusters identified in the analysis, as opposed to using a one size fits all approach.\n1. Importing the Data\nWe begin by loading the R packages we will need for the analysis. The dataset we have used for our example is publicly available – it’s the IBM Attrition dataset. You can download it here if you would like to follow along.\n\n\nlibrary(tidyverse) # data workhorse\nlibrary(readxl) # importing xlsx files\nlibrary(correlationfunnel) # rapid exploratory data analysis\nlibrary(cluster) # calculating gower distance and PAM\nlibrary(Rtsne) # dimensionality reduction and visualization\nlibrary(plotly) # interactive graphing\nlibrary(DT) # interactive tables\n\n\nset.seed(175) # reproducibility\n\nhr_data_tbl <- readxl::read_xlsx(\"clusteranalysis_files/datasets_1067_1925_WA_Fn-UseC_-HR-Employee-Attrition.xlsx\")\n\n\n\n2. Selecting Variables for Clustering\nUnder normal circumstances, we would spend time exploring the data – examining variables and their data types, visualising descriptive analyses (e.g., single variable and two variable analyses), understanding distributions, performing transformations if needed, and treating missing values and outliers.\nHowever, for the sake of simplicity, we will skip this and instead just calculate the correlation between attrition and each variable in the dataset. Those variables with a correlation of greater than 0.1 will be included in the analysis. This is useful for several reasons, but most importantly to decide which variables to include for our cluster analysis.\nKeep in mind that when it comes to clustering (or any other method), including more variables does not always mean a better outcome—simplicity should be the goal. Including more variables can complicate the interpretation of results and consequently make it difficult for the end-users to act upon results.\n\n\nhr_corr_tbl <- hr_data_tbl %>%\n    \n  # remove the Employee Number variable from the correlation analysis\n    dplyr::select(-EmployeeNumber) %>%\n    \n    # binarize cotinuous variables and one hot encode factors\n    correlationfunnel::binarize(n_bins        = 5, \n                                thresh_infreq = 0.01, \n                                name_infreq   = \"OTHER\", \n                                one_hot       = TRUE) %>%\n    \n    # correlate all variables with Attrition\n    correlationfunnel::correlate(Attrition__Yes)\n\n\nhr_corr_tbl %>%\n    # plot the correlation\n    correlationfunnel::plot_correlation_funnel(interactive = TRUE)\n\n\npreservee9567079d5901684\n\nBy using our correlation value of 0.1 as a cutoff, the analysis suggests the inclusion of fifteen variables for clustering: from overtime to business travel. In preparation for the analysis, any of these fifteen variables which are of a character data type (e.g. MaritalStatus = Single) are converted to a factor datatype (more on this below).\n\n\n# select the variables we wish to include in the analysis\nvar_selection <- c(\"EmployeeNumber\", \"Attrition\", \"OverTime\", \n                   \"JobLevel\", \"MonthlyIncome\", \"YearsAtCompany\", \n                   \"StockOptionLevel\", \"YearsWithCurrManager\", \n                   \"TotalWorkingYears\", \"MaritalStatus\", \"Age\", \n                   \"YearsInCurrentRole\", \"JobRole\", \"EnvironmentSatisfaction\",\n                   \"JobInvolvement\", \"BusinessTravel\")\n\n\nhr_subset_tbl <- hr_data_tbl %>%\n\n    # select our variables listed above\n    dplyr::select(one_of(var_selection)) %>%\n  \n    # convert the character variables to factors\n    dplyr::mutate_if(is.character, as_factor) %>%\n  \n    # recorder the variables\n    dplyr::select(EmployeeNumber, Attrition, everything())\n\n\n\n3. Analysis: Gower Distance\nIn essence, clustering is all about determining how similar (or dissimilar) cases in a dataset are to one another so that we can then group them together. To do this we first need to give each case (i.e., employee) a score based on the fifteen variables selected and then determine the difference between employees based on this score.\nThe most common way of performing this activity is by calculating the “Euclidean Distance”. However, Euclidean Distance is effective when analysing continuous variables (e.g., age, salary, tenure), and thus is not suitable for our HR dataset, which includes ordinal (e.g., EnvironmentSatisfaction – values from 1 = worst to 5 = best) and nominal data types (MaritalStatus – 1 = Single, 2 = Divorced, etc.). Therefore, we have to use a distance metric that can handle different data types; the Gower Distance.\nThe main advantage of Gower Distance is that it is simple to calculate and intuitive to understand. However, the disadvantage of this method is that it requires a distance matrix, a data structure that compares each case to every other case in the dataset, which needs considerable computing power and memory for large datasets.\n\n\n# Compute Gower distance and covert to a matrix\ngower_dist <- cluster::daisy(hr_subset_tbl[, 2:16], metric = \"gower\")\ngower_mat <- base::as.matrix(gower_dist)\n\n\n\nWe can perform a sanity check on our distance matrix by determining the most similar and/or dissimilar pair of employees. Here we calculate the two most similar employees according to their Gower Distance score.\n\n\n# Print most similar employees \nsimilar_tbl <- hr_subset_tbl[which(gower_mat == min(gower_mat[gower_mat != min(gower_mat)]), \n                                   arr.ind = TRUE)[1, ], ]\n\n\n# Transform the results to a datatable and save as a png file \nsimilar_employees <- similar_tbl %>%\n\n    # convert to a datatable\n    DT::datatable(rownames = FALSE, caption = 'Table of two most similar employees',\n                options = list(\n                                columnDefs = list(list(className = 'dt-center', \n                                                       targets   = 0:15)),\n                                dom            = 't',\n                                scrollX        = TRUE,\n                                scrollCollapse = TRUE)) \n\n\nsimilar_employees\n\n\npreserve41c6d72919006382\n\nWe learn from our sanity check in the table above that EmployeeID 1624 and EmployeeID 614, let’s call them Bob and Fred, are considered to be the most similar in the dataset. Looking at the table we can see that their values for fourteen of the fifteen variables are identical – only monthly salary differs slightly. The Gower Distance seems to be working and the output makes sense, now let’s perform the cluster analysis to see if we can understand turnover.\n4. Analysis: How many Clusters?\nThe one big question that must be answered when performing cluster analysis is “how many clusters should we segment the dataset into?”\nWe can use a data-driven approach to determine the optimal number of clusters by calculating the silhouette width. With this metric, we measure how similar each observation (i.e., in our case one employee) is to its assigned cluster, and conversely how dissimilar it is to neighbouring clusters. The metric can range from -1 to 1, where values of 1 show a clear cluster assignment, 0 suggests weak cluster assignment (i.e., a case could have been assigned to one of two neighbouring clusters) and -1 wrong cluster assignment.\n\n\n# perform the cluster analysis multiple times with different numbers of clusters\nsil_width <- purrr::map_dbl(2:10, function(k){\n  model <- cluster::pam(gower_dist, k = k)\n  model$silinfo$avg.width\n})\n\n\n# create a tibble with each row representing the cluster number \n# and average silhouette width\nsil_tbl <- tibble::tibble(\n  k = 2:10,\n  sil_width = sil_width\n)\n\n\n# visualize the clusters numbers and average silhouette width to \n# identify the optimal number of clusters\nfig2 <- ggplot2::ggplot(sil_tbl, aes(x = k, y = sil_width)) +\n  geom_point(size = 2) +\n  geom_line() +\n  scale_x_continuous(breaks = 2:10)\n\n\nplotly::ggplotly(fig2)\n\n\npreserve00a6142eaf6300d1\n\nTo identify the number of clusters with the highest average silhouette width, we perform the cluster analysis with differing numbers of clusters. In our case we choose two through to ten clusters. The reason we limited the maximum number of clusters to ten is that the larger numbers become the more difficult it becomes to interpret and ultimately act upon. As can be seen from the graph, six clusters generated the highest average silhouette width and will therefore be used in our analysis.\nIt is important to note that the average silhouette value (0.14) is actually quite low. According to anecdotal evidence, we would ideally want, at a minimum, a value between 0.25 and 0.5 (see Kaufmann and Rousseuw, 1990). Our low value might be indicative of limited structure in the data, or some clusters being weaker than others (i.e., some clusters group loosely, while others group tightly). To better understand the low value we have opted to look at the practical insights generated by the clusters and to visualise the cluster structure using t-Distributed Stochastic Neighbour Embedding (t-SNE). Both of these are explained below.\n5. Analysis: Interpreting the Clusters\nA topic we have not addressed yet, despite having already performed the clustering, is the method of cluster analysis employed.\nIn this analysis, we used the Partitioning Around Medoids (PAM) method. This method is identical to K-Means which is the most common form of clustering practiced. The only difference is that cluster centres for PAM are defined by the raw dataset observations, which in our example are our fifteen variables. This PAM approach has two key benefits over K-Means clustering. Firstly, it is less sensitive to outliers (e.g., such as a very high monthly income). Secondly, PAM also provides an exemplar case for each cluster, called a “Medoid”, which makes cluster interpretation easier.\nWith that this in mind, let’s re-run the cluster analysis with 6 clusters, as informed by our average silhouette width, join the cluster analysis results with our original dataset to identify into which cluster each individual falls, and then take a closer look at the six Medoids representing our six clusters.\n\n\n# set the number of clusters\nk <- 6\n\n\n# rerun the cluster analysis with 6 clusters\npam_fit <- cluster::pam(gower_dist, diss = TRUE, k)\n\n\n# add the cluster assignment to each employee\nhr_subset_tbl <- hr_subset_tbl %>%\n    dplyr::mutate(cluster = pam_fit$clustering) \n\n\n# have a look at the Medoids to understand the clusters\nhr_subset_tbl[pam_fit$medoids, ] %>% \n    \n    # add the cluster column to the front\n    dplyr::relocate(cluster, everything()) %>% \n    \n    # convert to a datatable, ensuring all rows are visible\n    DT::datatable(rownames = FALSE, caption = 'Cluster Medoids',\n                options = list(\n                                columnDefs = list(list(className = 'dt-center', \n                                                       targets   = 0:16)),\n                                dom            = 't',\n                                scrollX        = TRUE,\n                                scrollCollapse = TRUE)) \n\n\npreservea3f49fa89d10e717\n\nThe Medoids, which are the exemplar cases (i.e., employee) for each cluster, are presented in the table above. These Medoids can be useful in the creation of Persona’s if we were looking to operationlise results (discussed below).\nTo better understand attrition in our population we calculated the rate of attrition in each cluster, and how much each cluster captures overall attrition in our dataset.\n\n\nattrition_rate_tbl <- hr_subset_tbl %>%\n\n    # select our cluster number and attrition variables\n    dplyr::select(cluster, Attrition) %>%\n\n    # create a numeric representation of attrition\n    dplyr::mutate(attrition_num = forcats::fct_relevel(Attrition, \"No\", \"Yes\") %>% \n                    base::as.numeric() - 1) %>%\n\n    # group by cluster and then calculate cluster metrics\n    dplyr::group_by(cluster) %>%\n    dplyr::summarise(\n        Cluster_Turnover_Rate  = (base::sum(attrition_num) / base::length(attrition_num)) %>%\n                                  scales::percent(accuracy = 0.1),\n        Turnover_Count = base::sum(attrition_num),\n        Cluster_Size = base::length(attrition_num)\n    ) %>%\n    dplyr::ungroup() %>%\n\n    # calculate population metrics\n    dplyr::mutate(Population_Turnover_Rate = (Turnover_Count / base::sum(Turnover_Count)) %>%\n                                              scales::percent(accuracy = 0.1))\n\n\nattrition_rate_tbl %>% \n      \n    # convert to a datatable, ensuring all rows are visible\n    DT::datatable(rownames = FALSE, caption = 'Cluster Attrition Rates',\n                options = list(\n                                columnDefs = list(list(className = 'dt-center', \n                                                       targets   = 0:4)),\n                                dom = 't')) \n\n\npreserve103037c358dd96d6\n\nIt is now evident that almost 80% of employees in Cluster 3 left the organisation, which represents approximately 60% of all turnover recorded in the entire dataset. Great! Our identified clusters appear to generate some groupings clearly associated with turnover. Based on this we can gather a descriptive understanding of the combination of variables associated with turnover.\nUnfortunately, our code-based output up to this point is more attuned to data analysts than business partners and HR stakeholders. To make the results more digestible and actionable for non-analysts we will visualise them.\nOne way to collectively visualise the many variables from our cluster analysis together is with a method called t-SNE. This method is a dimensionality reduction technique that seeks to preserve the structure of the data while reducing it to two or three dimensions—something we can visualise! Technically, this step is not necessary but is recommended as it can be helpful in facilitating the understanding of results and thereby increasing the likelihood of action taken by stakeholders. In addition, it enables us to better understand the cluster structure, which was identified as weak by our average silhouette width metric.\nWe first create labels for our visualisation, perform the t-SNE calculation, and then visualise the t-SNE output.\n\n\n# create the labels for the t-SNE visualisation\ndata_formatted_tbl <- hr_subset_tbl %>%\n    dplyr::left_join(attrition_rate_tbl) %>%\n    dplyr::rename(Cluster = cluster) %>%\n    dplyr::mutate(MonthlyIncome = MonthlyIncome %>% scales::dollar()) %>%\n    dplyr::mutate(description = stringr::str_glue(\"Turnover = {Attrition}\n    \n                                  MaritalDesc = {MaritalStatus}\n                                  Age = {Age}\n                                  Job Role = {JobRole}\n                                  Job Level {JobLevel}\n                                  Overtime = {OverTime}\n                                  Current Role Tenure = {YearsInCurrentRole}\n                                  Professional Tenure = {TotalWorkingYears}\n                                  Monthly Income = {MonthlyIncome}\n                                  \n                                  Cluster: {Cluster}\n                                  Cluster Size: {Cluster_Size}\n                                  Cluster Turnover Rate: {Cluster_Turnover_Rate}\n                                  Cluster Turnover Count: {Turnover_Count} \n                                  \"))\n\n\n# map the clusters in 2 dimensional space using t-SNE\ntsne_obj <- Rtsne::Rtsne(gower_dist, is_distance = TRUE)\n\n\n# transform the rtsne object to a tibble and reformat data for visualization\ntsne_tbl <- tsne_obj$Y %>%\n    tibble::as_tibble() %>%\n    dplyr::rename(\"X\" = 1, \"Y\" = 2) %>%\n    dplyr::bind_cols(data_formatted_tbl) %>%\n    dplyr::mutate(Cluster = Cluster %>% forcats::as_factor())\n\n\n# visualize the data as a scatter plot using ggplot\ng <- tsne_tbl %>%\n    ggplot2::ggplot(aes(x = X, y = Y, colour = Cluster, text = description)) +\n    geom_point()\n\n\n# make the plot interactive with plotly\nplotly::ggplotly(g, tooltip = \"text\")\n\n\npreservedb10a2290a79d325\n\nThe plot allows us to graph our cluster analysis results in two dimensions, enabling end-users to visualise something that was previously code and concepts.\nAs suggested earlier by the average silhouette width metric (0.14), the grouping of the clusters is “serviceable”. There are some cases in each cluster that appear distant to the other cases in the cluster, but generally, the clusters appear to somewhat group together (Cluster 4 appears the weakest performer). On a practical note, it was reassuring that 80% of cases captured by Cluster 3 related to employee turnover, thereby enabling us to achieve our objective of better understanding attrition in this population.\nPersona’s\nWhen viewing the above visualisation we can hover over any dot in our visualisation and find out about its cluster membership, cluster turnover metrics, and the variable values associated with the employee. When we hover over cases in Cluster 3, we see variables associated with employees that are similar to our Cluster 3 Medoid, younger, scientific & sales professionals, with a few years of professional experience, minimal tenure in the company, and that left the company. We are beginning to develop a “Persona” associated with turnover, meaning that turnover is no longer a conceptual problem, it’s now a person we can characterise and understand.\nIdeally, this knowledge enables us to develop tailored interventions and strategies that improve the employee experience within the organisation and reduce the risk of unwanted turnover. The biggest benefit we gain from performing a cluster analysis as we just did is that intervention strategies are then applicable to a sizable group; the entire cluster, thus making it more cost-effective and impactful.\nIn addition, the analysis also shows us areas of the employee population where turnover is not a problem. This information may also be valuable when reviewing our employee offerings (e.g., policies and practices) and how well these offerings address turnover among our six clusters / personas. Furthermore, it can also influence the way in which we invest in future employee experience initiatives and our employee strategy in general.\nA final note\nIn this instance, we have demonstrated the process for performing cluster analysis (Unsupervised ML) using a mixture of data types to understand the topic of turnover in an HR dataset. Cluster analysis, while not a panacea for every HR problem, is a powerful method for understanding topics and people in HR, which can inform the way in which HR practitioners pragmatically scale personalised employee experiences and move away from a one size fits all approach.\nHappy Clustering!\nAcknowledgments\nThis article was first published on the Analytics In HR (AIHR) website under the title “A Tutorial on People Analytics Using R – Clustering”on June 13, 2020. The original version can be viewed here.\n\n\n\n",
    "preview": "posts/2020-08-04-clusteranalysis/sphx_glr_plot_cluster_comparison_0011.png",
    "last_modified": "2021-01-03T08:51:59+11:00",
    "input_file": "clusteranalysis.utf8.md",
    "preview_width": 2100,
    "preview_height": 1250
  },
  {
    "path": "posts/welcome/",
    "title": "A Beginner’s Guide to Machine Learning for HR Practitioners",
    "description": "An introductory article intended to demistify Machine Learning (ML)--an important subset of AI--for HR Professionals.",
    "author": [
      {
        "name": "Adam D McKinnon",
        "url": "https://www.adam-d-mckinnon.com/"
      },
      {
        "name": "Monica Ashton",
        "url": {}
      }
    ],
    "date": "2010-06-19",
    "categories": [
      "Artificial Intelligence",
      "Machine Learning",
      "HR"
    ],
    "contents": "\n\nContents\nWhat is AI?\nWhat is Machine Learning?\n1. What is Reinforcement Learning?\n2. What is Supervised Learning?\nAn example: Supervised Learning informing Employee Turnover\n\n3. What is Unsupervised Learning?\nAn example: Unsupervised Learning informing Employee Turnover\n\n\nConclusion\nAcknowledgments\n\n\n\n\n\nFigure 1: Image by Pietro Jeng on Unsplash.\n\n\n\nWhen you hear Artificial Intelligence (AI) the first thing that comes to mind are robots; in particular, the Steven Spielberg movie titled A.I. where a robot child is built that can love and behave just like a real human. This idea appears to be closer to a dream than reality. Truth is, AI is more ubiquitous than we might think. It ranges from self-driving cars, movie recommendations on Netflix, e-mail spam detection to voice-controlled assistants such as Apple’s SIRI. The fact is that AI is already present across many businesses and various industries, as is shown in the figure below (Note the low adoption rate in Human Resources).\n\n\n\nFigure 2: Image source: Where AI projects are being used within companies. From AI Adoption in the Enterprise, Magoulas, R. & Swoyer, S. (2020).\n\n\n\nStill, evidence suggests that HR departments remain unable to seize the multitude of opportunities associated with AI. In part, what may be required to accelerate the adoption of AI is educational content directed at HR Professionals, not data scientists. Thus, we offer this brief guide to Machine Learning (ML), an important subset of AI, with the intent to demystify ML and make it tangible.\nWhat is AI?\nLet’s start with the definition. AI is a broad area of computer science with the focus on building machines that can behave in an intelligent way—akin to humans. Furthermore, we can differentiate between 1. Generalized AI and 2. Specific AI. The concept behind Generalized AI is to develop machines that can perform multiple tasks, just like Spielberg’s robot-child. This is an area still in its infancy.\nMore relevant to HR is Specific AI, which refers to the use of intelligent machines to perform only one particular task, but to do it better than a human could (e.g. faster, more accurately, or objectively). For example, an application that reads hundreds of CVs in seconds and identifies optimal candidates for an interview—thereby affording HR professionals greater time to perform tasks humans do better than machines (e.g., building rapport, empathy, creativity, critical thinking, etc.).\nWhat is Machine Learning?\nBut how can a machine be programmed to process information (i.e., data) in an intelligent way? The answer to that is Machine Learning (ML). ML is a subset of Specific AI that comes from a mix of statistics and computer science. It refers to the process of computers learning to perform a task instead of following step-by-step instructions. This is generally performed iteratively by data scientists instructing a computer if its decisions are correct or incorrect. Depending on the outcome, the computer adapts how it makes decisions in the future—in other words, “it learns”.\n\n\n\nFigure 3: The relationship between AI, ML and the three broad types of ML.\n\n\n\nWhen it comes to ML there are basically three broad types:\nReinforcement Learning,\nUnsupervised Learning, and\nSupervised Learning.\n1. What is Reinforcement Learning?\nReinforcement Learning is probably best known through IBM’s Deep Blue computer, a “robot” that learned how to play chess and beat the human world champion.\nReinforcement Learning is a type of technique that enables an algorithm to learn by trial and error, using feedback from its own actions and experiences. Much like Pavlov and his dog, Reinforcement Learning involves rewarding decisions that lead to success and penalizing decisions that lead to anything other than success—ultimately making the algorithm more intelligent in the process.\nExamples of reinforcement learning applied in HR are a bit lean, though are most prevalent in areas such as education (i.e., applying content based on the progress of the student), finance and investment (i.e., advanced forecasting), supply chain operations (i.e., robots fulfilling orders in a warehouse), traffic flow optimization, and healthcare (i.e., accurate classification of biopsy images).\n2. What is Supervised Learning?\nThe most common forms of ML across industries, and specifically the HR domain, are Supervised Learning, followed by Unsupervised Learning.\nIn Supervised Learning, we try to predict an outcome, such as whether an employee will leave the company, the risk of an employee being injured, or the ideal starting salary of a new employee.\nTo make predictions we need different input variables (i.e., variables are called “Features” by data scientists). Our input features are only limited to our imagination (i.e., what we think will be important), what data we can get our hands-on, or what data we can create (e.g., by knowing where someone works and where they live we can create a variable focused on employee commute distance).\nAn example: Supervised Learning informing Employee Turnover\nLet’s examine a more detailed example of Supervised Learning—predicting who will leave an organization. Imagine that 1 in 5 new recruits leaves an organization in their first 12 months of tenure. To prevent such turnover, we could build a supervised learning model that predicts the likelihood of new starters leaving, so that our HR and managerial colleagues could intervene.\nIn this example, the model outcome being predicted is turnover risk, and the features used to predict turnover risk could include an employees’ demographic and employment characteristics (e.g., age, education level, role level, pay relative to market, month of employment, presence of development plans, and so on.).\nAssuming such a model was highly accurate, it would enable us to understand turnover among our new starter population from three angles.\nFirstly, what are the factors most influential in predicting turnover among our population. An example of such a model output is presented in the figure below, which illustrates whether a feature prevents turnover (green bars) or promotes turnover (red lines), and the relative importance of each feature in predicting turnover (i.e. longer lines denote more importance).\nSecondly, the model also rates the likelihood of each new starter leaving the company, enabling focused intervention (i.e. the risk that Adam will leave in his first 12 months).\nThirdly, the model identifies the features preventing or promoting turnover risk for each employee. This individualized output can enable HR professionals to take informed and personalized action, regardless of whether they personally know each employee.\n\n\n\nFigure 4: Example output from a model that predicts employee turnover risk. The plot describes the relative importance and directional influence of features in the model. Red bars represent features that contribute to employee turnover risk, while green bars represent features preventing turnover risk.\n\n\n\nA supervised learning model used to predict employee turnover among new starters has the potential to reduce notable costs, including financial (e.g., separation, vacancy, recruitment, training, and replacement) reputational (e.g., eroding an EVP and/or reducing candidate appeal) and productivity-related (e.g., on average organizations invest between four weeks and three months training new employees). Some of these costs can be readily quantified so that we can identify organizational savings based on prevented turnover (e.g., preventing 2 in 10 resignations saves $xxx).\n3. What is Unsupervised Learning?\nUnlike Supervised Learning where we are trying to predict an outcome, Unsupervised Learning analyzes many variables simultaneously to identify similarities, patterns or relationships in the data. Unsupervised Learning is more about understanding what’s in the data. The two most common uses of unsupervised learning are focused on:\nClustering: automatically splitting the dataset into groups based on similarities among the features analyzed. Classically applied to consumers, but equally relevant to organizations, whereby we understand our employee segments (i.e., clusters) and determine whether our HR policies serve the segments.\nAssociation mining: identifies sets of variables that often occur together in your dataset. For example, identifying injury patterns among workers at specific sites.\nAn example: Unsupervised Learning informing Employee Turnover\nCluster Analysis, the most famous form of unsupervised learning, can also help us better understand employee attrition. This approach can help group employees based on similar features (e.g., location, tenure, nationality, education level, age, performance level, etc.).\nThe figure below depicts the results of an analysis of the employee’s demographic features. Multiple demographic features are first reduced to two dimensions using a method called Manifold Learning (another non-supervised method), and these two new dimensions are then clustered using a method called T-SNE. The figure below shows us how the employees can be grouped together, in this case, twelve clusters, based on their demographic features.\n\n\n\nFigure 5: An example cluster analysis output when applied to employee features.\n\n\n\nOnce grouped into clusters, the next step is to determine the risk of turnover for each group. Moreover, it is interesting to identify if there are some shared risk factors, practically indicating that employees within a cluster are experiencing the workplace in a similar way.\nThis last insight is of considerable practical significance, as it may help us tailor interventions that target specific employee clusters, thereby delivering maximum impact (i.e., retaining employees and reducing turnover costs) and return on our investment (i.e. for every $ spent we generated $xxx in savings from reduced turnover).\nConclusion\nWe have begun to open the black box that is AI, providing a simple overview of ML. We looked at three broad types of ML—reinforcement, supervised and unsupervised—and examined some simple applications of each, where possible related to Human Resources. It is our genuine belief that through greater knowledge of what is possible with ML, HRBP’s and organizational decision-makers will both expect more, and be willing to do more, in this technological domain. Our next piece will explain the step-by-step process for performing Supervised ML—making discussions with People Analytics teams more tangible and less abstract!\nAcknowledgments\nThis article was first published on the Analytics In HR (AIHR) website under the title “A Beginner’s Guide to Machine Learning for HR Practitioners” on June 8th, 2020.\n\n\n\n",
    "preview": "posts/welcome/welcome_files/pietro-jeng-n6B49lTx7NM-unsplash.jpg",
    "last_modified": "2021-01-03T09:08:32+11:00",
    "input_file": "welcome.utf8.md"
  }
]
