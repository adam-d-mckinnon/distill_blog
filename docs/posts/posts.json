[
  {
    "path": "posts/2020-12-30-causalimpactanalysis/",
    "title": "Show Me The money! A People Analytics Guide to Measuring Impact Over Time",
    "description": "A \"How To\" article on the use of Causal Impact Analysis for People Analytics Practitioners.",
    "author": [
      {
        "name": "Adam D McKinnon",
        "url": "https://adam-d-mckinnon.com/"
      }
    ],
    "date": "2020-12-30",
    "categories": [
      "Causal Impact Analysis",
      "Impact Measurement",
      "Plotly",
      "R"
    ],
    "contents": "\n\nContents\nShow Me The Money!\nCausal Impact Analysis\nHow Does It Work?\n\nEnough Theory More Practice!\n1. Create Time Series Data\n2. Perform Causal Impact Analysis Using UK Data & Review Results\n3. Perform Causal Impact Analysis Using UK And Control Data & Review Results\n4. Quantifying The Financial Impact Of The Campaign\n\nHmmm Interesting But… Now What?\n\n\n\n\nFigure 1: Image source: Cubankite, Africa Studio, Dean Drobot - Shutterstock.\n\n\n\nShow Me The Money!\nWords immortalised by Tom Cruise in the 1996 move Jerry Maguire, yet at the heart of requests directed toward HR to quantify impact. However, quantifying impact is not always easy in HR—a field characterised by a sparsity of measurable KPIs such as product sales, conversions, defect rates, etc. While HR may not have the same measures, it can nonetheless borrow methods from neighbouring fields that deal in behavioural change and possess greater analytical maturity. To that end, this article is intended to introduce Causal Impact Analysis—a method initially developed in product marketing—to demonstrate a way to quantify impact in HR.\nCausal Impact Analysis\nCausal Impact Analysis is an approach to estimate “the causal effect of a designed intervention on a time series” (https://google.github.io/CausalImpact/CausalImpact.html). A HR related example would be the following—does a marketing campaign intended to get employees in the UK to reduce leave balances have an impact? Addressing questions like this can be challenging when controlled experimental conditions are not available. Kay Brodersen and team at Google built this algorithm to address this very challenge, and kindly open-sourced it for the rest of us to benefit from.\nHow Does It Work?\nLet’s use the HR example above–-does a marketing campaign launched in the UK that is intended to get employees to reduce leave balances have an impact? Causal Impact Analysis works by:\nexamining UK employee absences over time (i.e., a time series);\nexamining the absences in other, ideally comparable countries (i.e., a control time series); and\nbuilds a time-series model to try and predict how employee absences in the UK should have evolved without the campaign. The time series model can be built using either: a) only UK absence data, or b), using both UK data and the control time series (i.e., comparable countries).\nEnough Theory More Practice!\nThe following code provides a walkthrough of Causal Impact Analysis using R. The example context described above (i.e., quantifying the impact of a marketing campaign launched in the UK intended to get employees to reduce leave balances), will be played out below using fictitious data. The code plays out in the following way:\ncreate time series data\nperform causal impact analysis using UK data & review results\nperform causal impact analysis using UK and control data (i.e., employee absence in Italy, Sweden, Spain and Germany) & review results\nquantifying the financial impact of the campaign\n1. Create Time Series Data\nThe below code stub generates fictitious data for our example. Please note that you can replicate this example, however, the data generated may be different, which in turn may lead to different outcomes (i.e., both visualisations, summary statistics and benefit quantified).\n\n\n# load libraries\n\nlibrary(tidyverse) # workhorse\nlibrary(janitor) # better naming conventions\nlibrary(lubridate) # working with dates\nlibrary(zoo) # time series data format for causal impact\nlibrary(CausalImpact) # our method of interest\nlibrary(plotly) # making graphs attractive and interactive\n\n\nset.seed(1)\n\n# create a date sequence\n# takes default timezone setting from machine\nmydatetime <- as.POSIXct(\"2018-01-01\") \ndate <- base::seq.POSIXt(from = mydatetime, length.out = 365, by = \"day\")\n\n\n# control data - 4 countries (Italy, Sweden, Spain, Germany)\n# numbers reflect number of employees on leave each day\nit_numbers <- 100 + stats::arima.sim(model = list(ar = 0.999), n = 365) \nse_numbers <- 1.5 * it_numbers + rnorm(85)\nes_numbers <- 0.8 * it_numbers + rnorm (25)\nde_numbers <- 2.5 * it_numbers + rnorm (125)\n\n\n# case country data - UK\n# implementing a gradual increase to reflect realistic behaviour \n# (i.e., incremental absence/leave taking increase)\nuk_numbers          <- 1.2 * it_numbers + rnorm(100)\nuk_numbers[275:295] <- uk_numbers[275:365] + 2\nuk_numbers[296:316] <- uk_numbers[275:365] + 5\nuk_numbers[317:347] <- uk_numbers[275:365] + 12\nuk_numbers[348:365] <- uk_numbers[275:365] + 10\n\n\n# combine the data into one tibble\ndata_tbl <- bind_cols(date, uk_numbers, it_numbers, se_numbers, es_numbers, de_numbers) %>% \n            janitor::clean_names() %>% \n            dplyr::rename(date = x1,\n                   uk_numbers = x2,\n                   it_numbers = x3,\n                   se_numbers = x4,\n                   es_numbers = x5,\n                   de_numbers = x6) %>% \n            \n            # ONLY INCLUDE WEEKDAY DATA - no absences on weekend!\n            dplyr::mutate(date2 = date %>% lubridate::as_date() %>% lubridate::wday()) %>% \n\n            # 1 = sunday and 7 = saturday, 2 - 6 = weekdays\n            dplyr::filter(date2 >1 & date2 < 7) %>% \n\n            #  round numbers to whole numbers (no employee parts!)\n            dplyr::mutate_at(vars(dplyr::ends_with(\"_numbers\")), ~ base::round(., 0)) %>% \n            dplyr::select(-date2) \n\n\n# create UK only data\nuk_data <- data_tbl %>% \n            dplyr::select(1:2) %>% \n            base::as.data.frame() %>% \n            zoo::read.zoo()\n\n                              \n# create all countries data\nall_country_data <- data_tbl %>% \n                    base::as.data.frame() %>% \n                    zoo::read.zoo() \n\n\n\nWe now have our employee absence data for both the UK and our control countries of Italy, Sweden, Spain and Germany. Let’s begin by assessing the impact of the campaign when looking at the UK data in isolation.\n2. Perform Causal Impact Analysis Using UK Data & Review Results\nWe begin by specifying the pre and post campaign periods. With this established we are able to perform the causal impact analysis and then both visualise results and provide a statistical summary of the model.\n\n\n# establish when the intervention occurred\npre.period <- as.POSIXct(c(\"2018-01-01\", \"2018-09-30\"))\npost.period <- as.POSIXct(c(\"2018-10-01\", \"2018-12-31\"))\n\n\n# perform the causal impact analysis with UK data only\nimpact_uk_only <- CausalImpact::CausalImpact(data         = uk_data, \n                                              pre.period  = pre.period, \n                                              post.period = post.period)\n\n\n# get a graphical summary of the UK only-causal impact model\ngraphics::plot(impact_uk_only) %>% \n    plotly::plotly_build()\n\n\npreserve9affe57438d3497e\n\nThe graphical visualisation of the model provides three key perspectives of the model. The top panel of the visualisation shows a blue confidence interval, in which we would expect our time series data to remain in the absence of the campaign. The second panel displays the difference between situation normal (i.e., no campaign), which is depicted by the the zero value line in the graph, and what happened in reality. In our example we see the pointwise values occur both above and below the zero line. The third panel provides a perspective on the cumulative benefit of the campaign. The visualisation adds the pointwise contributions from the second panel, showing the benefit was minimal and only realised at the end of the post intervention period. None of these visualisation panels inspire confidence in our campaign!\n\n\n# get a summary &/or report of the model\nbase::summary(impact_uk_only) # summary results\n\n\nPosterior inference {CausalImpact}\n\n                         Average          Cumulative     \nActual                   107              7039           \nPrediction (s.d.)        106 (0.66)       6988 (43.74)   \n95% CI                   [104, 107]       [6895, 7070]   \n                                                         \nAbsolute effect (s.d.)   0.77 (0.66)      51.07 (43.74)  \n95% CI                   [-0.47, 2.2]     [-31.26, 144.0]\n                                                         \nRelative effect (s.d.)   0.73% (0.63%)    0.73% (0.63%)  \n95% CI                   [-0.45%, 2.1%]   [-0.45%, 2.1%] \n\nPosterior tail-area probability p:   0.11503\nPosterior prob. of a causal effect:  88%\n\nFor more details, type: summary(impact, \"report\")\n\n# plain text walk through of result - not run\n# base::summary(impact_uk_only, \"report\") # summary report\n\n\n\nThe summary results do not paint a compelling picture for our campaign either. I will call out a few key results and couple this with text lifted directly from the Summary Report (not included in this article due to its length, but a very useful explanation for Causal Impact Analysis users). The summary indicates that during the post-intervention period we had an actual average of 107 employees on leave per day. In the absence of the campaign we would have expected to have an average 106 employees on leave per day. This expectation is based on a time series model generated using pre-campaign UK data.\nOur 95% confidence interval tells us we would need the number to fall above 107 (or below 105 in case the campaign had an adverse impact) to be considered to have had an impact. The summary report goes into considerably more detail, and includes the following “although the intervention appears to have caused a positive effect, this effect is not statistically significant when considering the entire post-intervention period as a whole.” This interpretation is confirmed by our Posterior tail-area probability of p = 0.146. This means that the positive effect noted in the results may be by chance and is not considered statistically significant.\n3. Perform Causal Impact Analysis Using UK And Control Data & Review Results\nLet’s repeat the process, though this time including the data from the control countries (i.e., Italy, Sweden, Spain, and Germany. With our data, and pre and post campaign periods already established we can proceed directly to modelling and examining the output of the model.\n\n\n# perform the causal impact analysis with all data\nimpact_all_data <- CausalImpact::CausalImpact(data = all_country_data, \n                                              pre.period  = pre.period, \n                                              post.period = post.period)\n\n\n# get a graphical summary of the campaign impact\ngraphics::plot(impact_all_data) %>% \n    plotly::plotly_build()\n\n\npreservecd9db6e6a6dae65c\n\nThis time the visualisation of the model results are more compelling! The top panel shows a blue line which is our baseline. The baseline indicates where we would expect UK data to be in the absence of the campaign (based on both the control countries and UK data). The actual data appears to exceed the baseline. The second panel, depicting the difference between no campaign (i.e., the zero line) and the actual data indicated the campaign had a positive impact. The third panel suggests that the cumulative benefit of the campaign, based on the pointwise contributions from the second panel, is considerable. The campaign appears to be working, but let’s get a statistical perspective.\n\n\n# get a summary &/or report of the model\nbase::summary(impact_all_data) # summary results\n\n\nPosterior inference {CausalImpact}\n\n                         Average        Cumulative   \nActual                   107            7039         \nPrediction (s.d.)        100 (0.36)     6600 (23.90) \n95% CI                   [99, 101]      [6555, 6646] \n                                                     \nAbsolute effect (s.d.)   6.7 (0.36)     439.1 (23.90)\n95% CI                   [6, 7.3]       [393, 484.3] \n                                                     \nRelative effect (s.d.)   6.7% (0.36%)   6.7% (0.36%) \n95% CI                   [6%, 7.3%]     [6%, 7.3%]   \n\nPosterior tail-area probability p:   0.001\nPosterior prob. of a causal effect:  99.8997%\n\nFor more details, type: summary(impact, \"report\")\n\n# plain text walkthrough of result - not run\n# base::summary(impact_all_data, \"report\") # summary report\n\n\n\nThe results of the model suggest that the campaign had a positive impact that is statistically significant! Our actual average of 107 employees being on leave each day is above that of the model prediction, which was 100. In addition, our actual average is well above our 95% Confidence Interval of 101. Finally the probability that this result was due to chance alone is 0.001, and we have achieved a 99.8997% probability that the campaign had a causal effect (i.e., worth betting on!).\nComparing the two models (i.e., UK data only vs. UK data and control countries) clearly illustrates the value of including control data in this instance.\n4. Quantifying The Financial Impact Of The Campaign\nBased on the period we analysed (i.e., 1/10/2018 - 31/12/2018) our second model indicates that under normal circumstances we would have expected 100 employees on average to take leave each day, while the campaign seems to have catalysed a daily average of 107 employees. The cumulative benefit of this campaign is an additional 439 UK employees taking leave during the post campaign period (i.e., subtracting our cumulative prediction (6599) from our cumulative actual (7036)).\nIf the average daily cost of an UK employee is $300 GBP we could simply determine the financial benefit of the campaign by multiplying 439 by our average daily cost of an employee (i.e., $300 GBP). The estimated gross financial benefit of the campaign would be $131,700 GBP, which can be narrowed to a net financial benefit of $101,700 GBP, after subtracting our campaign costs of $30,000 GBP. Not bad HR!\nHmmm Interesting But… Now What?\nIn using Causal Impact Analysis, we can both:\nmeasure the ROI (Return-on-Investment) of behavioural campaigns (e.g. marketing campaign), especially when we are not sure if the campaign was the only single source of the impact; and\npotentially use Causal Impact Analysis to get feedback on impact quickly so that adjustments can be made in the moment, as opposed to when campaign momentum has been lost.\nIn addition, when tied with outcome metrics (e.g., average cost of unused leave) we can quantify the impact of interventions to demonstrate ROI to internal stakeholders. Causal Impact Analysis can be employed in a variety of settings—HR, Marketing, Communications, Finance, etc. The current article is intended to both provide a simple overview of the process of applying Causal Impact Analysis, and act as catalyst to future innovative applications in the People Analytics domain. Happy measuring!\n\n\n\n",
    "preview": "posts/2020-12-30-causalimpactanalysis/causalimpactanalysis_files/makeitrain.jpg",
    "last_modified": "2020-12-31T10:02:30+11:00",
    "input_file": "causalimpactanalysis.utf8.md"
  },
  {
    "path": "posts/welcome/",
    "title": "People Analytics 'How To' Blog",
    "description": "Welcome to our new blog, My Blog. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Adam McKinnon",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2020-12-30",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2020-12-30T16:32:25+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-10-forecastpeopleanalytics/",
    "title": "What is the Forecast for People Analytics?",
    "description": "An article on the use of Google Trends and the new TimeTK and ModelTime Libraries in R.",
    "author": [
      {
        "name": "Adam D McKinnon",
        "url": "https://adam-d-mckinnon.com/"
      },
      {
        "name": "Monica Ashton",
        "url": {}
      }
    ],
    "date": "2020-10-10",
    "categories": [
      "Forecasting",
      "TimeTK",
      "ModelTime",
      "Plotly",
      "R"
    ],
    "contents": "\n\nContents\nThe goal is to know what’s coming…\nData Collection\nModeling\nStep 1.\nStep 2\nStep 3.\nStep 4.\n\nImplications of Forecasting in HR\nWorkforce Planning\nTalent Acquisition\nOutsourcing\nFinancial budgeting\nAcknowledgments\n\n\n\n\n\nThe last 9 months have, more than ever, emphasized the importance of knowing what is coming. In this article, we take a closer look at forecasting. Forecasting can be applied to a range of HR-related topics. We will specifically examine how forecasting models can be deployed in R, using an example analysis on the rise in popularity of “people analytics”.\nThe goal is to know what’s coming…\nPredictions come in different shapes and sizes. There are many Supervised Machine Learning algorithms that can generate predictions of outcomes, such as flight risk, safety incidents, performance and engagement outcomes, and personnel selection. These examples represent the highly popular realm of “Predictive Analytics”.\nHowever, a less mainstream topic in the realm of prediction is that of “Forecasting” – often referred to as Time Series Analysis. In a nutshell, forecasting takes values over time (e.g., closing price of a stock over 120 days) to forecast the likely value in the future.\nThe main difference between supervised machine learning and forecasting is best characterized by the data used. Generally, forecasting relies upon historical data, and the patterns identified therein, to predict future values.\nAn HR-related example would be using historical rates of attrition in a business or geography to forecast future rates of attrition. In contrast, predictive analytics uses a variety of additional variables, such as company performance metrics, economic indicators, employment data, and so on, to predict future rates of turnover. Depending upon the use case, there is a time and a place for both approaches.\nIn the current article, we focus on forecasting and highlight a new library in the R ecosystem called ModelTime. ModelTime enables the application of multiple forecasting models quickly and easily while employing a tidy framework (for those not familiar with R don’t worry about this).\nTo illustrate the ease of using ModelTime we forecast the future level of interest in the domain of People Analytics using Google Trends data. From there we will discuss potential applications of forecasting supply and demand in the context of HR.\nData Collection\nThe time-series data we will use for our example comes directly from Google Trends. Google Trends is an online tool that enables users to discover trends in search behavior within Google Search, Google News, Google Images, Google Shopping, and YouTube.\nTo do so, users are required to specify the following:\nA search term (up to four additional comparison search terms are optional),\nA geography (i.e., where the Google Searches were performed),\nA time period, and\nGoogle source for searches (e.g., Web Search, Image Search, News Search, Google Shopping, or YouTube Search).\nIt is important to note that the search data returned does NOT represent the actual search volume in numbers, but rather a normalized index ranging from 0-100. The values returned represent the search interest relative to the highest search interest during the time period selected. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular at that point in time. A score of 0 means there was not enough data for this term.\n\n\n# Libraries\nlibrary(gtrendsR)\nlibrary(tidymodels)\nlibrary(modeltime)\nlibrary(tidyverse)\nlibrary(timetk)\nlibrary(lubridate)\nlibrary(flextable)\nlibrary(prophet)\n\n\n# Data - Google Trends Setup\nsearch_term   <- \"people analytics\"\nlocation      <- \"\" # global\ntime          <- \"2010-01-01 2020-08-01\" # format \"Y-m-d Y-m-d\"\ngprop         <- \"web\"\n\n\n# Get Google Trends Data\ngtrends_result_list <- gtrendsR::gtrends(\n    keyword = search_term, \n    geo     = location, \n    time    = time,\n    gprop   = gprop\n    )\n\n\n# Data Cleaning\ngtrends_search_tbl <- gtrends_result_list %>%\n    purrr::pluck(\"interest_over_time\") %>%\n    tibble::as_tibble() %>%\n    dplyr::select(date, hits) %>%\n    dplyr::mutate(date = ymd(date)) %>%\n    dplyr::rename(value = hits)\n\n\n# Visualise the Google Trends Data\ngtrends_search_tbl %>%\n  timetk::plot_time_series(date, value)\n\n\npreserved9dd6d13dd73a47a\n\nWe can see from the visualisation that the term “people analytics” has trended upwards in Google web searches from January 2010 through to August 2020. The blue trend line, established using a LOESS smoother (i.e., a non-parametric technique that tries to find a curve of best fit without assuming the data adheres to a specific distribution) illustrates a continual rise in interest. The raw data also indicates that the Google search term of “people analytics”, perhaps unsurprisingly, peaked in June of 2020.\nThis peak may relate to the impact of COVID-19, specifically the requirement for organisations to deliver targeted ad-hoc reporting on personnel topics during this time. Irrespective, the future for People Analytics seems to be of increasing importance.\nModeling\nLet’s move into some Forecasting! The process employed using ModelTime is as follows:\nWe separate our dataset into “Training” and “Test” datasets. The Training data represents that data from January 2010 to July 2019, while the Test data represents the last 12 months of data (i.e., August 2019 – August 2020). A visual representation of this split is presented in the image you see below.\nThe Training data is used to generate a 12-month forecast using several different models. In this article, we have chosen the following models: Exponential Smoothing, ARIMA, ARIMA Boost, Prophet, and Prophet Boost.\nThe forecasts generated are then compared to the Test data (i.e., actual data) to determine the accuracy of the different models.\nBased on the accuracy of the different models, one or more models are then applied to the entire dataset (i.e., Jan 2010 – August 2020) to provide a forecast into 2021.\n\n\n\nWe have presented the R code below, with supporting outputs, for steps 1 through to 4.\nStep 1.\n\n\n# Train/Test \n# hold out 12 months for test split\nmonths <- 12\n\n\n# calculate the total numnber of months in data\ntotal_months <- lubridate::interval(base::min(gtrends_search_tbl$date),\n                                    base::max(gtrends_search_tbl$date)) %/%  \n                                    base::months(1)\n\n\n# determine the proportion for the test split\nprop <- (total_months - months) / total_months\n\n\n# Train/Test Split\nsplits <- rsample::initial_time_split(gtrends_search_tbl, prop = prop)\n\n\n# Plot splits as sanity check\nsplits %>%\n  timetk::tk_time_series_cv_plan() %>%  \n  timetk::plot_time_series_cv_plan(date, value) \n\n\npreserve1cc2b1ec3c3c7743\n\nThe plot above visually depicts our Training and Testing splits in the data, specifically the time period represented by both.\nStep 2\nWe first generate a 12-month forecast using five models (Exponential Smoothing, ARIMA, ARIMA Boost, Prophet, and Prophet Boost) on the training data.\n\n\n# Modeling\n# Exponential Smoothing\nmodel_fit_ets <- modeltime::exp_smoothing() %>%\n    parsnip::set_engine(engine = \"ets\") %>%\n    parsnip::fit(value ~ date, data = training(splits))\n\n\n# ARIMA \nmodel_fit_arima <- modeltime::arima_reg() %>%\n    parsnip::set_engine(\"auto_arima\") %>%\n    parsnip::fit(\n        value ~ date, \n        data = training(splits))\n\n\n# ARIMA Boost\nmodel_fit_arima_boost <- modeltime::arima_boost() %>%\n    parsnip::set_engine(\"auto_arima_xgboost\") %>%\n    parsnip::fit(\n        value ~ date + as.numeric(date) + month(date, label = TRUE), \n        data = training(splits))\n\n\n# Prophet\nmodel_fit_prophet <- modeltime::prophet_reg() %>%\n    parsnip::set_engine(\"prophet\") %>%\n    parsnip::fit(\n        value ~ date, \n        data = training(splits))\n\n\n# Prophet Boost\nmodel_fit_prophet_boost <- modeltime::prophet_boost() %>%\n    parsnip::set_engine(\"prophet_xgboost\") %>%\n    parsnip::fit(\n        value ~ date + as.numeric(date) + month(date, label = TRUE), \n        data = training(splits))\n\n\n# Modeltime Table\nmodel_tbl <- modeltime::modeltime_table(\n    model_fit_ets,\n    model_fit_arima,\n    model_fit_arima_boost,\n    model_fit_prophet,\n    model_fit_prophet_boost)\n\n\n\nStep 3.\nThe forecasts generated above are now compared to the Test data (i.e., actual data) to determine the accuracy of the different models.\n\n\n# Calibrate the model accuracy using the test data\ncalibration_tbl <- model_tbl %>%\n    modeltime::modeltime_calibrate(testing(splits))  \n\n\n# create a table of the calibration results\ncalibration_tbl %>%\n    modeltime::modeltime_accuracy() %>%   \n    flextable::flextable() %>% \n    flextable::bold(part = \"header\") %>% \n    flextable::bg(bg = \"#D3D3D3\", part = \"header\") %>% \n    flextable::autofit()\n\n\npreserve7390448e04b471c9\n\nThe table above illustrates the metrics derived when evaluating the accuracy of the respective models using the Test set. While it is beyond the scope of this article to explain the models and their metrics, a simple rule of thumb when looking at the below table is that smaller error numbers generally indicate a better model!\nThe graph below illustrates how the models performed relative to the actual data (i.e., our Test set).\n\n\n# plot the forecast\ncalibration_tbl %>%\n    modeltime::modeltime_forecast(new_data = testing(splits), \n                                  actual_data = gtrends_search_tbl,\n                                  conf_interval = 0.90) %>%\n    modeltime::plot_modeltime_forecast(.legend_show = TRUE, \n                                       .legend_max_width = 20)\n\n\npreserve332729d9679de9c8\n\nStep 4.\nBased on these metrics we decided to use all five models to forecast into 2021 (i.e., our final Step 4).\n\n\n# Refit the five models with the last 12 months of data (i.e., Test data)\nrefit_tbl <- calibration_tbl %>%\n    modeltime::modeltime_refit(data = gtrends_search_tbl) \n\n\n# forecast the next 12 months into 2021\nforecast_tbl <- refit_tbl %>%\n    modeltime::modeltime_forecast(\n        h = \"1 year\",\n        actual_data = gtrends_search_tbl,\n        conf_interval = 0.90\n    ) \n\n\n# 3 create an interactive visualization of the forecast\nforecast_tbl %>%\n    modeltime::plot_modeltime_forecast(.interactive = TRUE,\n                                       .legend_max_width = 20)\n\n\npreserve8c926246fd0f4bea\n\nTo enhance the accuracy and interpretability (particularly among stakeholders) of the forecast we will take an average of the five models to create an aggregate model. We can see below in both the code and the interactive visualization, that the ongoing trend for people analytics is one of increasing popularity over time!\n\n\n# Create an aggregated model based on our 5 models\nmean_forecast_tbl <- forecast_tbl %>%\n    dplyr::filter(.key != \"actual\") %>%\n    dplyr::group_by(.key, .index) %>%\n    dplyr::summarise(across(.value:.conf_hi, mean)) %>%\n    dplyr::mutate(\n        .model_id   = 6,\n        .model_desc = \"AVERAGE OF MODELS\"\n    )\n\n\n# Visualize aggregate model \nforecast_tbl %>%\n    dplyr::filter(.key == \"actual\") %>%\n    dplyr::bind_rows(mean_forecast_tbl) %>%\n    modeltime::plot_modeltime_forecast()  \n\n\npreservec10359b50c36e767\n\nThe forecast of Google Search interest for the next 12 months appears to continue its trend of upward growth – the forecast for People Analytics seems bright!\nImplications of Forecasting in HR\nThe above example illustrates the ease with which analysts can perform forecasting in R with time-series data to be better prepared for the future. In addition, the use of automated models (i.e., those that self-optimize) can be an excellent entry point for forecasting. Technologies such as ModelTime in R enable users to rapidly and easily apply numerous sophisticated forecasting models to perform scenario planning within organisations.\nScenario planning need not be something that is performed once and later shelved to collect dust, despite varying environmental conditions. In the realm of HR, forecasting can and should be readily repeated to play a crucial, and yet often-underutilized part of strategic activities such as the following:\nWorkforce Planning\nWhat proportion of the employee population is likely to retire over the coming 2 – 5 years? How many employees will the organization need to replace in the future?\nWill the local job market or universities “produce” sufficient candidates to cover an organisations forecasted graduate/ professional employee recruitment needs?\nWhen opening new facilities in new markets, is the local population sufficient to support our employee requirements?\nAre there job profile areas where we are likely to experience talent shortfalls in the near to medium-term future?\nWhat is the trend in specific skills as captured by online job boards? Of the skills that your organisation values / requires, what do you have, what do you forecast needing in light of varying environmental conditions?\nTalent Acquisition\nHow many employees are we likely to recruit in the next 2 – 4 quarters to meet business goals?\nHow many talent acquisition staff will be required in specific geographies to meet seasonal recruiting requirements (which do vary by geography!)?\nOutsourcing\nBased on the historical outsourcing activities, what is the current trend and what are the financial implications associated with that requirement?\nWill the outsourcing provider be able to cater to future demand requirements? Request forecasts from vendors to substantiate their proposals.\nBased on turnover among outsourced roles, what are the future implications for onboarding and training needs in specific businesses/geographies? How many L&D staff will be required to support those demands?\nFinancial budgeting\nWhat are the future budget requirements for HR activities?\nWhat is the future financial requirement associated with establishing a people analytics team? ;-)\nThe above list, while far from comprehensive, provides a sense of the multitude of ways in which forecasting can be applied in HR to make data driven decisions.\nHappy Forecasting!\nAcknowledgments\nThe authors would like to acknowledge the work of Matt Dancho, both in the development and maintenance of TimeTK and ModelTime, and the Learning Labs Pro Series ran by Matt, upon which this article is directly based.\nThis article was first published on the Analytics In HR (AIHR) website under the title “Forecasting in R: a People Analytics Tool” on August 31st, 2020.\n\n\n\n",
    "preview": "posts/2020-10-10-forecastpeopleanalytics/forecasting-market-icon.jpg",
    "last_modified": "2020-12-31T12:08:04+11:00",
    "input_file": "forecastpeopleanalytics.utf8.md"
  }
]
